{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wJPK-WGie-Y"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGFjCweNie-a"
   },
   "source": [
    "# Quickstart with RAGStack\n",
    "\n",
    "This notebook demonstrates how to set up a simple RAG pipeline with RAGStack. At the end of this notebook, you will have a fully functioning Question/Answer model that can answer questions using your supplied documents.\n",
    "\n",
    "A RAG pipeline requires, at minimum, a vector store, an embedding model, and an LLM. In this tutorial, you will use an Astra DB vector store, an OpenAI embedding model, an OpenAI LLM, and LangChain to orchestrate it all together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z75psDlBie-b"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "You will need a vector-enabled Astra database and an OpenAI Account.\n",
    "\n",
    "* Create an [Astra vector database](https://docs.datastax.com/en/astra-serverless/docs/getting-started/create-db-choices.html).\n",
    "* Create an [OpenAI account](https://openai.com/)\n",
    "* Within your database, create an [Astra DB Access Token](https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html) with Database Administrator permissions.\n",
    "* Get your Astra DB Endpoint:\n",
    "  * `https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com`\n",
    "\n",
    "See the [Prerequisites](https://docs.datastax.com/en/ragstack/docs/prerequisites.html) page for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3O84KE8ie-c"
   },
   "source": [
    "## Setup\n",
    "`ragstack-ai` includes all the packages you need to build a RAG pipeline.\n",
    "\n",
    "`datasets` is used to import a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "id": "m3zJeojNie-c",
    "nbmake": {
     "post_cell_execute": [
      "from conftest import before_notebook",
      "before_notebook()"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet ragstack-ai datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.4 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.56 which is incompatible.\n",
      "langchain-community 0.0.16 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.56 which is incompatible.\n",
      "ragstack-ai 0.7.0 requires astrapy<0.8.0,>=0.7.0, but you have astrapy 1.1.0 which is incompatible.\n",
      "ragstack-ai 0.7.0 requires langchain-core==0.1.16, but you have langchain-core 0.1.52 which is incompatible.\n",
      "langchain-experimental 0.0.57 requires langchain<0.2.0,>=0.1.15, but you have langchain 0.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain_astradb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "id": "DrXGpDLVie-d",
    "nbmake": {
     "post_cell_execute": [
      "import string\n",
      "import random\n",
      "collection = ''.join(random.choice(string.ascii_lowercase) for _ in range(8))\n"
     ]
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter your settings for Astra DB and OpenAI:\n",
    "os.environ[\"ASTRA_DB_API_ENDPOINT\"] = \"ASTRA_DB_API_ENDPOINT\"\n",
    "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = \"ASTRA_DB_APPLICATION_TOKEN\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROoi1Speie-d"
   },
   "source": [
    "## Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ2sPEUNie-d"
   },
   "source": [
    "### Embedding Model and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "id": "mYu_5WPFie-e",
    "outputId": "9c19e038-97d5-476d-ca45-4959da68a782",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.miniconda3/envs/graph/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astra vector store configured\n"
     ]
    }
   ],
   "source": [
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "# Configure your embedding model and vector store\n",
    "embedding = OpenAIEmbeddings()\n",
    "vstore = AstraDBVectorStore(\n",
    "    collection_name=\"test\",\n",
    "    embedding=embedding,\n",
    "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
    ")\n",
    "print(\"Astra vector store configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UBDZ4QAgie-e",
    "outputId": "9844c23b-5b47-45f6-dc27-dbfd6b9464a0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80951e0872494a41968781c60e1024fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847be6355847423882d37d01221b9957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/67.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f51ef3363c4e78a02081fab3dc7d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example entry:\n",
      "{'author': 'aristotle', 'quote': 'Love well, be loved and do something of value.', 'tags': 'love;ethics'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sample dataset\n",
    "philo_dataset = load_dataset(\"datastax/philosopher-quotes\")[\"train\"]\n",
    "print(\"An example entry:\")\n",
    "print(philo_dataset[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>quote</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aristotle</td>\n",
       "      <td>True happiness comes from gaining insight and ...</td>\n",
       "      <td>knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aristotle</td>\n",
       "      <td>The roots of education are bitter, but the fru...</td>\n",
       "      <td>education;knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aristotle</td>\n",
       "      <td>Before you heal the body you must first heal t...</td>\n",
       "      <td>ethics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aristotle</td>\n",
       "      <td>The proof that you know something is that you ...</td>\n",
       "      <td>education;knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aristotle</td>\n",
       "      <td>Those who are not angry at the things they sho...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author                                              quote  \\\n",
       "0  aristotle  True happiness comes from gaining insight and ...   \n",
       "1  aristotle  The roots of education are bitter, but the fru...   \n",
       "2  aristotle  Before you heal the body you must first heal t...   \n",
       "3  aristotle  The proof that you know something is that you ...   \n",
       "4  aristotle  Those who are not angry at the things they sho...   \n",
       "\n",
       "                  tags  \n",
       "0            knowledge  \n",
       "1  education;knowledge  \n",
       "2               ethics  \n",
       "3  education;knowledge  \n",
       "4                 None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.DataFrame(philo_dataset)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8oYsSPotie-e"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Constructs a set of documents from your data. Documents can be used as inputs to your vector store.\n",
    "docs = []\n",
    "for entry in philo_dataset:\n",
    "    metadata = {\"author\": entry[\"author\"]}\n",
    "    if entry[\"tags\"]:\n",
    "        # Add metadata tags to the metadata dictionary\n",
    "        for tag in entry[\"tags\"].split(\";\"):\n",
    "            metadata[tag] = \"y\"\n",
    "    # Create a LangChain document with the quote and metadata tags\n",
    "    doc = Document(page_content=entry[\"quote\"], metadata=metadata)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rZ6vOYZEie-f",
    "nbmake": {
     "post_cell_execute": [
      "assert len(inserted_ids) > 0"
     ]
    },
    "outputId": "be972e76-4f63-41f7-b5ba-d58422e8f331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserted 450 documents.\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings by inserting your documents into the vector store.\n",
    "inserted_ids = vstore.add_documents(docs)\n",
    "print(f\"\\nInserted {len(inserted_ids)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "id": "yDTUk66Uie-f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checks your collection to verify the documents are embedded.\n",
    "#print(vstore.astra_db.collection(\"test\").find())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1CGP0UEie-f"
   },
   "source": [
    "### Basic Retrieval\n",
    "\n",
    "Retrieve context from your vector database, and pass it to the model with a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "W5snPjIBie-f",
    "outputId": "1e950019-b692-48e8-d374-1fabb8ee9bdd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.miniconda3/envs/graph/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Philosophers are most concerned with knowledge and truth.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "retriever = vstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"In the given context, what subject are philosophers most concerned with?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_FMcVo3zie-f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The most important thing to allow the brain is the full measure of sleep required to restore it. The tags provided in the context are: author ('schopenhauer'), ethics ('y'), and knowledge ('y').\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add your questions here!\n",
    "chain.invoke(\"In the given context, what is the most important things to  allow the brain and provide me teh tags?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NLai_3jie-f"
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_5KrpZ7ie-f",
    "nbmake": {
     "post_cell_execute": [
      "# Deletes collection for test suite to allow each test to run with a fresh collection",
      "vstore.delete_collection()"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: This will delete the collection and all documents in the collection\n",
    "# vstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJ7W_FcBie-g"
   },
   "source": [
    "You now have a fully functioning RAG pipeline! Note that there are several different ways to accomplish this, depending on your input data format, vector store, embedding, model, output type, and more. There are also more advanced RAG techniques that leverage new ingestion, retrieval, and generation patterns.  \n",
    "\n",
    "RAG is a powerful solution used in tandem with the capabilities of LLMs. Check out our other examples for ideas on how you can build innovative solutions using RAGStack!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
